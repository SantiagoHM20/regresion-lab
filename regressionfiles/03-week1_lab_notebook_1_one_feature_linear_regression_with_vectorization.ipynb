{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503fbd7e",
   "metadata": {},
   "source": [
    "\n",
    "# Week 1 Lab â€“ Linear Regression with One Feature\n",
    "\n",
    "In this lab we will work with a very simple version of supervised learning:\n",
    "\n",
    "- Our data consists of pairs $(x^{(i)}, y^{(i)})$, where $x^{(i)}$ is one feature and $y^{(i)}$ is the target.\n",
    "- We will use a **linear regression model** with one feature.\n",
    "- We will measure how good the model is using a **cost function** based on mean squared error.\n",
    "- We will train the model with **gradient descent**.\n",
    "- This file uses vectorization in python. If you are not familiar with vectorization you can start with \n",
    "the notebook of similar name but that do not use vectorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0d9d78",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Theory Refresher\n",
    "\n",
    "### 0.1 Linear Regression (One Feature)\n",
    "\n",
    "We assume there is (approximately) a linear relationship between the input $x$ and the output $y$.  \n",
    "Our model (or hypothesis) is a function that depends on the parameters $w$ and $b$:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x) = wx + b\n",
    "$$\n",
    "\n",
    "- $w$ is the **slope**: how much $f_{w,b}(x)$ changes when $x$ increases by 1.\n",
    "- $b$ is the **intercept**: the value of $f_{w,b}(x)$ when $x = 0$.\n",
    "- For a dataset with $m$ examples, we write the $i$-th example as $(x^{(i)}, y^{(i)})$.  \n",
    "  The prediction for that example is:\n",
    "  $$\n",
    "  \\hat{y}^{(i)} = f_{w,b}(x^{(i)}) = w x^{(i)} + b\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d6988",
   "metadata": {},
   "source": [
    "\n",
    "### 0.2 Cost Function (Mean Squared Error)\n",
    "\n",
    "We need a way to measure how well a particular line (given by $w$ and $b$) fits the data.\n",
    "\n",
    "We use the **mean squared error (MSE)** cost function:\n",
    "\n",
    "$$\n",
    "J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\big( \\hat{y}^{(i)} - y^{(i)} \\big)^2\n",
    "       = \\frac{1}{2m} \\sum_{i=1}^{m} \\big( f_{w,b}(x^{(i)}) - y^{(i)} \\big)^2\n",
    "$$\n",
    "\n",
    "- The term $(\\hat{y}^{(i)} - y^{(i)})$ is the **error** for example $i$.\n",
    "- We square the error so that positive and negative errors do not cancel out, and to penalize large errors more.\n",
    "- The factor $\\frac{1}{2m}$ is for mathematical convenience when taking derivatives.\n",
    "\n",
    "Our goal is to find values of $w$ and $b$ that **minimize** $J(w,b)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02af1be6",
   "metadata": {},
   "source": [
    "\n",
    "### 0.3 Gradient Descent\n",
    "\n",
    "To minimize $J(w,b)$, we use **gradient descent**.  \n",
    "The idea is to start with some initial $(w,b)$ and repeatedly update them in the direction that decreases the cost.\n",
    "\n",
    "We compute the partial derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( f_{w,b}(x^{(i)}) - y^{(i)} \\big) x^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( f_{w,b}(x^{(i)}) - y^{(i)} \\big)\n",
    "$$\n",
    "\n",
    "Given a **learning rate** $\\alpha > 0$, we update:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J}{\\partial w}, \\qquad\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "We repeat these updates many times. If $\\alpha$ is chosen well, the cost $J(w,b)$ will decrease and $(w,b)$ will move toward values that fit the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930b6fe4",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required libraries (run this once if needed)\n",
    "%pip install numpy pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c2679",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2c7d6",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Create or Load a Simple Dataset\n",
    "\n",
    "We will create a synthetic dataset that roughly follows a linear relationship:\n",
    "\n",
    "$$\n",
    "y \\approx 3 x + 2 + \\text{noise}\n",
    "$$\n",
    "\n",
    "Each point is a pair $(x^{(i)}, y^{(i)})$ with **one feature** $x^{(i)}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f41da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = 50\n",
    "\n",
    "x = np.linspace(0, 10, m)\n",
    "\n",
    "true_w = 3.0\n",
    "true_b = 2.0\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "noise = rng.normal(loc=0.0, scale=2.0, size=m)\n",
    "\n",
    "y = true_w * x + true_b + noise\n",
    "\n",
    "print(f\"Number of examples m = {m}\")\n",
    "print(\"First 5 x values:\", x[:5])\n",
    "print(\"First 5 y values:\", y[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8795c5",
   "metadata": {},
   "source": [
    "### 2.1 Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd61536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Dataset: one feature x vs target y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e7f65",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Linear Regression Model with One Feature\n",
    "\n",
    "We use the model (hypothesis function):\n",
    "\n",
    "$$\n",
    "f_{w,b}(x^{(i)}) = w x^{(i)} + b\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $w$ is the slope,\n",
    "- $b$ is the intercept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dbd394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(x, w, b):\n",
    "    \"\"\"Compute the predicted y values for given x, using f_{w,b}(x) = w x + b.\"\"\"\n",
    "    return w * x + b\n",
    "\n",
    "w_test = 0.0\n",
    "b_test = 0.0\n",
    "y_hat_test = predict(x, w_test, b_test)\n",
    "print(\"First 5 predictions with w=0, b=0:\", y_hat_test[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0550e3a3",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Cost Function $J(w,b)$\n",
    "\n",
    "We define the **mean squared error** cost function:\n",
    "\n",
    "$$\n",
    "J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\big( f_{w,b}(x^{(i)}) - y^{(i)} \\big)^2\n",
    "$$\n",
    "\n",
    "This measures how well the model $f_{w,b}(x) = w x + b$ fits the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d0c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cost(x, y, w, b):\n",
    "    m = x.shape[0]\n",
    "    y_hat = w * x + b  # f_{w,b}(x)\n",
    "    errors = y_hat - y\n",
    "    cost = (1 / (2 * m)) * np.sum(errors ** 2)\n",
    "    return cost\n",
    "\n",
    "print(\"Cost with w=0, b=0:\", compute_cost(x, y, w_test, b_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1688eb49",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Visualize the Cost Function as a Surface\n",
    "\n",
    "We can visualize how $J(w,b)$ changes as we vary $w$ and $b$.  \n",
    "Below we plot the **cost surface** $J(w,b)$ in 3D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a23d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "w_values = np.linspace(2.5, 3.5, 1000)\n",
    "b_values = np.linspace(0.5, 1.5, 1000)\n",
    "\n",
    "W, B = np.meshgrid(w_values, b_values)\n",
    "J_vals = np.zeros_like(W)\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        J_vals[i, j] = compute_cost(x, y, W[i, j], B[i, j])\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(W, B, J_vals, cmap=cm.viridis, linewidth=0, antialiased=True)\n",
    "ax.set_xlabel(\"w\")\n",
    "ax.set_ylabel(\"b\")\n",
    "ax.set_zlabel(\"J(w,b)\")\n",
    "ax.set_title(\"Cost surface J(w,b)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7398ec2",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Gradient Descent\n",
    "\n",
    "We use **gradient descent** with the update rules:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( f_{w,b}(x^{(i)}) - y^{(i)} \\big) x^{(i)}, \\quad\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( f_{w,b}(x^{(i)}) - y^{(i)} \\big)\n",
    "$$\n",
    "\n",
    "Update:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J}{\\partial w}, \\quad\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd78ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_gradients(x, y, w, b):\n",
    "    m = x.shape[0]\n",
    "    y_hat = w * x + b  # f_{w,b}(x)\n",
    "    errors = y_hat - y\n",
    "\n",
    "\n",
    "    dj_dw = (1 / m) * np.sum(errors * x)\n",
    "    dj_db = (1 / m) * np.sum(errors)\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "dj_dw_test, dj_db_test = compute_gradients(x, y, w_test, b_test)\n",
    "print(\"Gradients at w=0, b=0:\", dj_dw_test, dj_db_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edd02e0",
   "metadata": {},
   "source": [
    "### 5.1 Implement the Gradient Descent Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b20e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(x, y, w_init, b_init, alpha, num_iterations):\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    history = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        dj_dw, dj_db = compute_gradients(x, y, w, b)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        cost = compute_cost(x, y, w, b)\n",
    "        history.append((i, cost))\n",
    "\n",
    "        if i % max(1, (num_iterations // 10)) == 0:\n",
    "            print(f\"Iteration {i:4d}: w={w:7.4f}, b={b:7.4f}, cost={cost:8.4f}\")\n",
    "\n",
    "    return w, b, history\n",
    "\n",
    "alpha = 0.01\n",
    "num_iterations = 2000\n",
    "\n",
    "w_init = 0.0\n",
    "b_init = 0.0\n",
    "\n",
    "w_learned, b_learned, history = gradient_descent(x, y, w_init, b_init, alpha, num_iterations)\n",
    "print(\"\\nLearned parameters:\")\n",
    "print(\"w =\", w_learned)\n",
    "print(\"b =\", b_learned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561425f1",
   "metadata": {},
   "source": [
    "### 5.2 Plot the Cost over Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iterations = [it for it, c in history]\n",
    "costs = [c for it, c in history]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(iterations[15:], costs[15:])  # skip the first points\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost J(w,b)\")\n",
    "plt.title(\"Gradient Descent: Cost vs Iterations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035f67c",
   "metadata": {},
   "source": [
    "### 5.3 Visualize the Fitted Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7079da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, label=\"Data\")\n",
    "y_pred = predict(x, w_learned, b_learned)\n",
    "plt.plot(x, y_pred, label=\"Fitted line\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Linear Regression Fit (one feature)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5795a550",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Exercises (for You to Try)\n",
    "\n",
    "1. **Change the learning rate $\\alpha$**:\n",
    "   - Try values like `0.001`, `0.1`, `0.5`.\n",
    "   - What happens to the speed of convergence? Does the algorithm diverge for some values?\n",
    "\n",
    "2. **Change the number of iterations**:\n",
    "   - Try `num_iterations = 100`, `500`, `2000`.\n",
    "   - How does the final cost change?\n",
    "\n",
    "3. **Try different initial values** for `w_init` and `b_init`:\n",
    "   - Does gradient descent still converge to similar values?\n",
    "\n",
    "4. **Noise level**:\n",
    "   - Go back to the cell where we define `noise` and change `scale` (e.g., `scale=0.5` or `scale=5.0`).\n",
    "   - How does the fitted line look with less/more noise?\n",
    "\n",
    "5. **(Optional) Manual check**:\n",
    "   - Pick some values of $w$ and $b$, compute $J(w,b)$ using `compute_cost`,\n",
    "   - Plot the line and see visually if a smaller cost corresponds to a better fit.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
