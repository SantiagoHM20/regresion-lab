{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b583965",
   "metadata": {},
   "source": [
    "\n",
    "# Week 1 Lab â€“ Linear Regression with One Feature (No Vectorization)\n",
    "\n",
    "In this lab we will work with a very simple version of supervised learning:\n",
    "\n",
    "- Our data consists of pairs $(x^{(i)}, y^{(i)})$, where $x^{(i)}$ is one feature and $y^{(i)}$ is the target.\n",
    "- We will use a **linear regression model** with one feature.\n",
    "- We will measure how good the model is using a **cost function** based on mean squared error.\n",
    "- We will train the model with **gradient descent**.\n",
    "- In all code, we will avoid NumPy vectorization and use **explicit Python loops**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046014d",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Theory Refresher\n",
    "\n",
    "### 0.1 Linear Regression (One Feature)\n",
    "\n",
    "We assume there is (approximately) a linear relationship between the input $x$ and the output $y$.  \n",
    "Our model (or hypothesis) is a function that depends on the parameters $w$ and $b$:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x) = wx + b\n",
    "$$\n",
    "\n",
    "- $w$ is the **slope**: how much $f_{w,b}(x)$ changes when $x$ increases by 1.\n",
    "- $b$ is the **intercept**: the value of $f_{w,b}(x)$ when $x = 0$.\n",
    "- For a dataset with $m$ examples, we write the $i$-th example as $(x^{(i)}, y^{(i)})$.  \n",
    "  The prediction for that example is:\n",
    "  $$\n",
    "  \\hat{y}^{(i)} = f_{w,b}(x^{(i)}) = w x^{(i)} + b\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ddeae8",
   "metadata": {},
   "source": [
    "\n",
    "### 0.2 Cost Function (Mean Squared Error)\n",
    "\n",
    "We need a way to measure how well a particular line (given by $w$ and $b$) fits the data.\n",
    "\n",
    "We use the **mean squared error (MSE)** cost function:\n",
    "\n",
    "$$\n",
    "J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\big( \\hat{y}^{(i)} - y^{(i)} \\big)^2\n",
    "       = \\frac{1}{2m} \\sum_{i=1}^{m} \\big( f_{w,b}(x^{(i)}) - y^{(i)} \\big)^2\n",
    "$$\n",
    "\n",
    "- The term $(\\hat{y}^{(i)} - y^{(i)})$ is the **error** for example $i$.\n",
    "- We square the error so that positive and negative errors do not cancel out, and to penalize large errors more.\n",
    "- The factor $\\frac{1}{2m}$ is for mathematical convenience when taking derivatives.\n",
    "\n",
    "Our goal is to find values of $w$ and $b$ that **minimize** $J(w,b)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff3cf3",
   "metadata": {},
   "source": [
    "\n",
    "### 0.3 Gradient Descent\n",
    "\n",
    "To minimize $J(w,b)$, we use **gradient descent**.  \n",
    "The idea is to start with some initial $(w,b)$ and repeatedly update them in the direction that decreases the cost.\n",
    "\n",
    "We compute the partial derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( f_{w,b}(x^{(i)}) - y^{(i)} \\big) x^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( f_{w,b}(x^{(i)}) - y^{(i)} \\big)\n",
    "$$\n",
    "\n",
    "Given a **learning rate** $\\alpha > 0$, we update:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J}{\\partial w}, \\qquad\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "We repeat these updates many times. If $\\alpha is chosen well, the cost $J(w,b)$ will decrease and $(w,b)$ will move toward values that fit the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d056d5",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0492632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required libraries (run this once if needed)\n",
    "%pip install numpy pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4152c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034975d",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Create a Simple Dataset\n",
    "\n",
    "We will create a synthetic dataset that roughly follows a linear relationship:\n",
    "\n",
    "$$\n",
    "y \\approx 3 x + 2 + \\text{noise}\n",
    "$$\n",
    "\n",
    "Each point is a pair $(x^{(i)}, y^{(i)})$ with **one feature** $x^{(i)}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c698fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = 50\n",
    "\n",
    "# Use numpy only to generate evenly spaced values, then convert to plain Python list\n",
    "x_array = np.linspace(0, 10, m)\n",
    "x = [float(v) for v in x_array]\n",
    "\n",
    "true_w = 3.0\n",
    "true_b = 2.0\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "noise_array = rng.normal(loc=0.0, scale=2.0, size=m)\n",
    "noise = [float(v) for v in noise_array]\n",
    "\n",
    "# Build y using explicit loops\n",
    "y = []\n",
    "for i in range(m):\n",
    "    y_value = true_w * x[i] + true_b + noise[i]\n",
    "    y.append(y_value)\n",
    "\n",
    "print(f\"Number of examples m = {m}\")\n",
    "print(\"First 5 x values:\", x[:5])\n",
    "print(\"First 5 y values:\", y[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f818ac4",
   "metadata": {},
   "source": [
    "### 2.1 Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c327f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Dataset: one feature x vs target y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9e741",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Linear Regression Model with One Feature\n",
    "\n",
    "We use the model (hypothesis function):\n",
    "\n",
    "$$\n",
    "f_{w,b}(x^{(i)}) = w x^{(i)} + b\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $w$ is the slope,\n",
    "- $b$ is the intercept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9672b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(x_list, w, b):\n",
    "    \"\"\"Compute predicted y values for a list of x, using f_{w,b}(x) = w x + b.\"\"\"\n",
    "    y_hat_list = []\n",
    "    for i in range(len(x_list)):\n",
    "        y_hat_list.append(w * x_list[i] + b)\n",
    "    return y_hat_list\n",
    "\n",
    "w_test = 0.0\n",
    "b_test = 0.0\n",
    "y_hat_test = predict(x, w_test, b_test)\n",
    "print(\"First 5 predictions with w=0, b=0:\", y_hat_test[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e04d0",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Cost Function $J(w,b)$\n",
    "\n",
    "We define the **mean squared error** cost function:\n",
    "\n",
    "$$\n",
    "J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\big( f_{w,b}(x^{(i)}) - y^{(i)} \\big)^2\n",
    "$$\n",
    "\n",
    "This measures how well the model $f_{w,b}(x) = w x + b$ fits the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a44ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cost(x_list, y_list, w, b):\n",
    "    \"\"\"Compute the cost J(w,b) using explicit loops.\"\"\"\n",
    "    m_local = len(x_list)\n",
    "    total = 0.0\n",
    "    for i in range(m_local):\n",
    "        f_wb = w * x_list[i] + b\n",
    "        diff = f_wb - y_list[i]\n",
    "        total += diff * diff\n",
    "    cost = total / (2 * m_local)\n",
    "    return cost\n",
    "\n",
    "print(\"Cost with w=0, b=0:\", compute_cost(x, y, w_test, b_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714f849",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Visualize the Cost Function as a Surface\n",
    "\n",
    "We can visualize how $J(w,b)$ changes as we vary $w$ and $b$.  \n",
    "Below we plot the **cost surface** $J(w,b)$ in 3D using explicit loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c708d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D  # needed to register the 3D projection\n",
    "from matplotlib import cm\n",
    "\n",
    "# Choose reasonable ranges around the expected optimum\n",
    "w_values = [float(v) for v in np.linspace(-1.0, 7.0, 60)]\n",
    "b_values = [float(v) for v in np.linspace(-5.0, 10.0, 60)]\n",
    "\n",
    "# Build W, B, J_vals as lists of lists using explicit loops\n",
    "W = []\n",
    "B = []\n",
    "J_vals = []\n",
    "\n",
    "for i in range(len(b_values)):  # rows: b\n",
    "    row_W = []\n",
    "    row_B = []\n",
    "    row_J = []\n",
    "    for j in range(len(w_values)):  # cols: w\n",
    "        w_curr = w_values[j]\n",
    "        b_curr = b_values[i]\n",
    "        row_W.append(w_curr)\n",
    "        row_B.append(b_curr)\n",
    "        j_val = compute_cost(x, y, w_curr, b_curr)\n",
    "        row_J.append(j_val)\n",
    "    W.append(row_W)\n",
    "    B.append(row_B)\n",
    "    J_vals.append(row_J)\n",
    "\n",
    "W_arr = np.array(W)\n",
    "B_arr = np.array(B)\n",
    "J_arr = np.array(J_vals)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.plot_surface(W_arr, B_arr, J_arr, cmap=cm.viridis, linewidth=0, antialiased=True)\n",
    "ax.set_xlabel(\"w\")\n",
    "ax.set_ylabel(\"b\")\n",
    "ax.set_zlabel(\"J(w,b)\")\n",
    "ax.set_title(\"Cost surface J(w,b)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e8801",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Gradient Descent\n",
    "\n",
    "We use **gradient descent** with the update rules:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( f_{w,b}(x^{(i)}) - y^{(i)} \\big) x^{(i)}, \\quad\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( f_{w,b}(x^{(i)}) - y^{(i)} \\big)\n",
    "$$\n",
    "\n",
    "Update:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J}{\\partial w}, \\quad\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_gradients(x_list, y_list, w, b):\n",
    "    \"\"\"Compute dJ/dw and dJ/db using explicit loops.\"\"\"\n",
    "    m_local = len(x_list)\n",
    "    sum_dw = 0.0\n",
    "    sum_db = 0.0\n",
    "\n",
    "    for i in range(m_local):\n",
    "        f_wb = w * x_list[i] + b\n",
    "        error = f_wb - y_list[i]\n",
    "        sum_dw += error * x_list[i]\n",
    "        sum_db += error\n",
    "\n",
    "    dj_dw = sum_dw / m_local\n",
    "    dj_db = sum_db / m_local\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "dj_dw_test, dj_db_test = compute_gradients(x, y, w_test, b_test)\n",
    "print(\"Gradients at w=0, b=0:\", dj_dw_test, dj_db_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af4cec",
   "metadata": {},
   "source": [
    "### 5.1 Implement the Gradient Descent Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(x_list, y_list, w_init, b_init, alpha, num_iterations):\n",
    "    \"\"\"Run gradient descent using explicit loops for gradients and cost.\"\"\"\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    history_iterations = []\n",
    "    history_costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        dj_dw, dj_db = compute_gradients(x_list, y_list, w, b)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        cost = compute_cost(x_list, y_list, w, b)\n",
    "        history_iterations.append(i)\n",
    "        history_costs.append(cost)\n",
    "\n",
    "        if i % max(1, (num_iterations // 10)) == 0:\n",
    "            print(f\"Iteration {i:4d}: w={w:7.4f}, b={b:7.4f}, cost={cost:8.4f}\")\n",
    "\n",
    "    return w, b, history_iterations, history_costs\n",
    "\n",
    "alpha = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "w_init = 1.0\n",
    "b_init = 1.0\n",
    "\n",
    "w_learned, b_learned, it_hist, cost_hist = gradient_descent(x, y, w_init, b_init, alpha, num_iterations)\n",
    "print(\"\\nLearned parameters:\")\n",
    "print(\"w =\", w_learned)\n",
    "print(\"b =\", b_learned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa7691",
   "metadata": {},
   "source": [
    "### 5.2 Plot the Cost over Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c7410",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(it_hist[10:], cost_hist[10:])  # skip the first pointplt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost J(w,b)\")\n",
    "plt.title(\"Gradient Descent: Cost vs Iterations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df347f",
   "metadata": {},
   "source": [
    "### 5.3 Visualize the Fitted Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c10ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, label=\"Data\")\n",
    "y_pred = predict(x, w_learned, b_learned)\n",
    "plt.plot(x, y_pred, label=\"Fitted line\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Linear Regression Fit (one feature)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e3c92",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Exercises (for You to Try)\n",
    "\n",
    "1. **Change the learning rate $\\alpha$**:\n",
    "   - Try values like `0.001`, `0.1`, `0.5`.\n",
    "   - What happens to the speed of convergence? Does the algorithm diverge for some values?\n",
    "\n",
    "2. **Change the number of iterations**:\n",
    "   - Try `num_iterations = 100`, `500`, `2000`.\n",
    "   - How does the final cost change?\n",
    "\n",
    "3. **Try different initial values** for `w_init` and `b_init`:\n",
    "   - Does gradient descent still converge to similar values?\n",
    "\n",
    "4. **Noise level**:\n",
    "   - Go back to the cell where we define `noise_array` and change `scale` (e.g., `scale=0.5` or `scale=5.0`).\n",
    "   - How does the fitted line look with less/more noise?\n",
    "\n",
    "5. **(Optional) Manual check**:\n",
    "   - Pick some values of $w$ and $b$, compute $J(w,b)$ using `compute_cost`,\n",
    "   - Plot the line and see visually if a smaller cost corresponds to a better fit.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
