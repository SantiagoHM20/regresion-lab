{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78543a1b",
   "metadata": {},
   "source": [
    "\n",
    "# Week 1 Lab – Part 2: Multiple Features and Polynomial Regression (House Prices)\n",
    "\n",
    "In this second part of the lab we will extend linear regression from **one feature** to **multiple features**, and then to **polynomial regression**, using **house prices** as a running example.\n",
    "\n",
    "We keep the same notation as in class:\n",
    "\n",
    "- The $i$-th training example is $(x^{(i)}, y^{(i)})$.\n",
    "- Each input $x^{(i)}$ has $n$ features: \n",
    "  $$\n",
    "  x^{(i)} = \\big[x_1^{(i)}, x_2^{(i)}, \\dots, x_n^{(i)}\\big].\n",
    "  $$\n",
    "- The model (hypothesis) is a function $f_{w,b}(x)$ that depends on parameters $w$ and $b$:\n",
    "  $$\n",
    "  f_{w,b}(x) = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b.\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c83f0",
   "metadata": {},
   "source": [
    "## 0.1 Linear Regression with Multiple Features\n",
    "\n",
    "For one feature we had\n",
    "$$\n",
    "f_{w,b}(x) = wx + b.\n",
    "$$\n",
    "\n",
    "For **multiple features**, with $\\vec{x} \\in \\mathbb{R}^n$ and $\\vec{w} \\in \\mathbb{R}^n$, we write\n",
    "$$\n",
    "f_{\\vec{w}, b}(\\vec{x}) = \\sum_{j=1}^{n} w_j x_j + b.\n",
    "$$\n",
    "\n",
    "In **vector notation** this becomes\n",
    "$$\n",
    "f_{\\vec{w}, b}(\\vec{x}) = \\vec{w}^T \\vec{x} + b.\n",
    "$$\n",
    "\n",
    "For a whole dataset with $m$ examples, we collect the inputs into a **design matrix** $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$:\n",
    "- Row $i$ of $\\mathbf{X}$ is $(x_1^{(i)}, \\dots, x_n^{(i)})$.\n",
    "- The target vector is $\\vec{y} \\in \\mathbb{R}^m$, with entries $y^{(i)}$.\n",
    "\n",
    "Then the predictions for all examples at once are\n",
    "$$\n",
    "\\hat{\\vec{y}} = \\mathbf{X} \\vec{w} + b \\,\\vec{1},\n",
    "$$\n",
    "where $\\vec{1}$ is the vector of all ones (size $m$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdabfb54",
   "metadata": {},
   "source": [
    "## 0.2 Cost Function for Multiple Features\n",
    "\n",
    "The cost function is the same **mean squared error** we used before, now written for the multivariate case:\n",
    "\n",
    "$$\n",
    "J(\\vec{w}, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\right)^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5145b3fe",
   "metadata": {},
   "source": [
    "## 0.3 Gradient Descent for Multiple Features\n",
    "\n",
    "The partial derivatives of $J(\\vec{w}, b)$ are:\n",
    "\n",
    "- For each $w_j$:\n",
    "  $$\n",
    "  \\frac{\\partial J(\\vec{w}, b)}{\\partial w_j} \n",
    "  = \\frac{1}{m} \\sum_{i=1}^{m} \\big( f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\big)\\, x_j^{(i)}.\n",
    "  $$\n",
    "\n",
    "- For the bias term $b$:\n",
    "  $$\n",
    "  \\frac{\\partial J(\\vec{w}, b)}{\\partial b} \n",
    "  = \\frac{1}{m} \\sum_{i=1}^{m} \\big( f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\big).\n",
    "  $$\n",
    "\n",
    "In **vector/matrix form**, let\n",
    "- $\\hat{\\vec{y}} = f_{\\vec{w}, b}(\\mathbf{X})$ (the vector of predictions),\n",
    "- $\\vec{e} = \\hat{\\vec{y}} - \\vec{y}$ (the error vector).\n",
    "\n",
    "Then:\n",
    "$$\n",
    "\\nabla_{\\vec{w}} J(\\vec{w}, b) \n",
    "= \\frac{1}{m} \\,\\mathbf{X}^T \\vec{e},\n",
    "\\qquad\n",
    "\\frac{\\partial J(\\vec{w}, b)}{\\partial b}\n",
    "= \\frac{1}{m} \\sum_{i=1}^{m} e^{(i)}.\n",
    "$$\n",
    "\n",
    "The gradient descent updates are:\n",
    "$$\n",
    "\\vec{w} := \\vec{w} - \\alpha \\, \\nabla_{\\vec{w}} J(\\vec{w}, b),\n",
    "\\qquad\n",
    "b := b - \\alpha \\, \\frac{\\partial J(\\vec{w}, b)}{\\partial b},\n",
    "$$\n",
    "where $\\alpha$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19a4c4",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541ce667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required libraries (run this once if needed)\n",
    "%pip install numpy pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f1d13",
   "metadata": {},
   "source": [
    "\n",
    "## 2. A Simple House Prices Dataset\n",
    "\n",
    "We will simulate a small house-price dataset with the following features:\n",
    "\n",
    "- $x_1$ = `size_m2` (size of the house in square meters),\n",
    "- $x_2$ = `bedrooms` (number of bedrooms),\n",
    "- $x_3$ = `front_m` (front of the lot in meters),\n",
    "- $x_4$ = `depth_m` (depth of the lot in meters).\n",
    "\n",
    "The target $y$ is the house price (in thousands of dollars).\n",
    "\n",
    "We will create data that roughly follows a linear relationship plus some noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of examples\n",
    "m = 80\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# Features\n",
    "size_m2 = rng.uniform(50, 200, m)        # 50 to 200 m^2\n",
    "bedrooms = rng.integers(1, 5, m)         # 1 to 4 bedrooms\n",
    "front_m = rng.uniform(5, 15, m)          # 5 to 15 m\n",
    "depth_m = rng.uniform(8, 25, m)          # 8 to 25 m\n",
    "\n",
    "# \"True\" underlying parameters (unknown to the algorithm)\n",
    "true_w = np.array([1.5, 10.0, 3.0, 2.0])  # impact of each feature on price\n",
    "true_b = 50.0                             # base price in thousands\n",
    "noise = rng.normal(0, 10, m)              # noise in thousands\n",
    "\n",
    "# Build design matrix X and target y\n",
    "X = np.column_stack([size_m2, bedrooms, front_m, depth_m])\n",
    "y = X @ true_w + true_b + noise\n",
    "\n",
    "# Put into a DataFrame just to inspect\n",
    "data = pd.DataFrame({\n",
    "    \"size_m2\": size_m2,\n",
    "    \"bedrooms\": bedrooms,\n",
    "    \"front_m\": front_m,\n",
    "    \"depth_m\": depth_m,\n",
    "    \"price_k\": y,\n",
    "})\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108120ed",
   "metadata": {},
   "source": [
    "### 2.1 Visualize Size vs Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.scatter(data[\"size_m2\"], data[\"price_k\"])\n",
    "plt.xlabel(\"Size (m^2)\")\n",
    "plt.ylabel(\"Price (thousands)\")\n",
    "plt.title(\"House size vs price\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d14d73",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Vectorized Model Implementation\n",
    "\n",
    "We now implement the model and cost function using **NumPy arrays** and **vectorized operations**.\n",
    "\n",
    "### 3.1 Hypothesis $f_{w,b}(x)$\n",
    "\n",
    "For a dataset with matrix $X$ and parameters $w$ and $b$, the vector of predictions is:\n",
    "$$\n",
    "\\hat{y} = X w + b.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d761bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(X, w, b):\n",
    "    \"\"\"Compute predictions f_{w,b}(x) for all examples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (m, n)\n",
    "        Design matrix: each row is x^(i).\n",
    "    w : np.ndarray, shape (n,)\n",
    "        Parameter vector.\n",
    "    b : float\n",
    "        Bias term.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_hat : np.ndarray, shape (m,)\n",
    "        Vector of predictions for each example.\n",
    "    \"\"\"\n",
    "    return X @ w + b  # vectorized: matrix-vector product + scalar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b18573",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Cost Function $J(w,b)$ (Vectorized)\n",
    "\n",
    "We recall\n",
    "$$\n",
    "J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\big( f_{w,b}(x^{(i)}) - y^{(i)} \\big)^2.\n",
    "$$\n",
    "\n",
    "In vector form, if we define the error vector $e = \\hat{y} - y$, then\n",
    "$$\n",
    "J(w,b) = \\frac{1}{2m} e^T e.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd910cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"Compute the cost J(w,b) for linear regression with multiple features.\n",
    "\n",
    "    Uses the vectorized formula:\n",
    "        J = (1 / (2m)) * (y_hat - y)^T (y_hat - y)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (m, n)\n",
    "    y : np.ndarray, shape (m,)\n",
    "    w : np.ndarray, shape (n,)\n",
    "    b : float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cost : float\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    y_hat = predict(X, w, b)\n",
    "    error = y_hat - y\n",
    "    cost = (error @ error) / (2 * m)\n",
    "    return cost\n",
    "\n",
    "# Test with w = 0, b = 0\n",
    "n = X.shape[1]\n",
    "w_test = np.zeros(n)\n",
    "b_test = 0.0\n",
    "print(\"Cost with w=0, b=0:\", compute_cost(X, y, w_test, b_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b6174",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Gradient of the Cost Function (Vectorized)\n",
    "\n",
    "We derived:\n",
    "$$\n",
    "\\nabla_w J(w,b) = \\frac{1}{m} X^T (\\hat{y} - y), \\qquad\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}).\n",
    "$$\n",
    "\n",
    "We now implement this directly in NumPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99960b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"Compute the gradients of J with respect to w and b.\n",
    "\n",
    "    Vectorized formulas:\n",
    "        dj_dw = (1/m) * X^T (y_hat - y)\n",
    "        dj_db = (1/m) * sum(y_hat - y)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    y_hat = predict(X, w, b)\n",
    "    error = y_hat - y\n",
    "\n",
    "    dj_dw = (X.T @ error) / m\n",
    "    dj_db = np.sum(error) / m\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "dj_dw_test, dj_db_test = compute_gradient(X, y, w_test, b_test)\n",
    "print(\"Gradient at w=0, b=0:\")\n",
    "print(\"dj_dw:\", dj_dw_test)\n",
    "print(\"dj_db:\", dj_db_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0fbcbe",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Gradient Descent for Multiple Features\n",
    "\n",
    "We now put everything together into a gradient descent loop.\n",
    "\n",
    "At each iteration:\n",
    "1. Compute $\\hat{y} = f_{w,b}(X)$.\n",
    "2. Compute the gradients $\\nabla_w J$ and $\\partial J / \\partial b$.\n",
    "3. Update\n",
    "   $$\n",
    "   w := w - \\alpha \\nabla_w J, \\qquad\n",
    "   b := b - \\alpha \\frac{\\partial J}{\\partial b}.\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ad976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(X, y, w_init, b_init, alpha, num_iterations):\n",
    "    \"\"\"Run gradient descent to learn w and b.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (m, n)\n",
    "    y : np.ndarray, shape (m,)\n",
    "    w_init : np.ndarray, shape (n,)\n",
    "    b_init : float\n",
    "    alpha : float\n",
    "        Learning rate.\n",
    "    num_iterations : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w : np.ndarray, shape (n,)\n",
    "    b : float\n",
    "    history_it : list of int\n",
    "    history_cost : list of float\n",
    "    \"\"\"\n",
    "    w = w_init.copy()\n",
    "    b = b_init\n",
    "    history_it = []\n",
    "    history_cost = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        dj_dw, dj_db = compute_gradient(X, y, w, b)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        if i % 10 == 0 or i == num_iterations - 1:\n",
    "            cost = compute_cost(X, y, w, b)\n",
    "            history_it.append(i)\n",
    "            history_cost.append(cost)\n",
    "            print(f\"Iteration {i:4d}: cost = {cost:8.4f}\")\n",
    "\n",
    "    return w, b, history_it, history_cost\n",
    "\n",
    "alpha = 1e-4\n",
    "num_iterations = 1000\n",
    "w_init = np.zeros(n)\n",
    "b_init = 0.0\n",
    "\n",
    "w_learned, b_learned, it_hist, cost_hist = gradient_descent(X, y, w_init, b_init, alpha, num_iterations)\n",
    "print(\"\\nLearned parameters:\")\n",
    "print(\"w =\", w_learned)\n",
    "print(\"b =\", b_learned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e518e",
   "metadata": {},
   "source": [
    "### 5.1 Plot the Cost over Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a272fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(it_hist[15:], cost_hist[15:])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost J(w,b)\")\n",
    "plt.title(\"Gradient Descent: Cost vs Iterations (multiple features)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(it_hist, cost_hist)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost J(w,b) (log scale)\")\n",
    "plt.title(\"Gradient Descent: Cost vs Iterations (log scale)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965932e9",
   "metadata": {},
   "source": [
    "### 5.2 Compare Predicted vs True Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7d94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = predict(X, w_learned, b_learned)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y, y_pred)\n",
    "plt.xlabel(\"True price (thousands)\")\n",
    "plt.ylabel(\"Predicted price (thousands)\")\n",
    "plt.title(\"Predicted vs True Prices\")\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()])  # diagonal line\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852875f5",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Feature Scaling\n",
    "\n",
    "Gradient descent can be **slow** if the features have very different scales.\n",
    "\n",
    "For example, in our dataset:\n",
    "- `size_m2` might be around 100–200,\n",
    "- `bedrooms` is around 1–4,\n",
    "- `front_m` and `depth_m` have yet other ranges.\n",
    "\n",
    "A common solution is to **scale** each feature, for example using **standardization**:\n",
    "$$\n",
    "x_j^{(i)} \\leftarrow \\frac{x_j^{(i)} - \\mu_j}{\\sigma_j},\n",
    "$$\n",
    "where $\\mu_j$ is the mean of feature $j$, and $\\sigma_j$ its standard deviation.\n",
    "\n",
    "We will implement a simple feature scaling function and compare convergence **with** and **without** scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d610d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_scale(X):\n",
    "    \"\"\"Standardize each feature to have mean 0 and standard deviation 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_scaled : np.ndarray, shape (m, n)\n",
    "    means : np.ndarray, shape (n,)\n",
    "    stds : np.ndarray, shape (n,)\n",
    "    \"\"\"\n",
    "    means = X.mean(axis=0)\n",
    "    stds = X.std(axis=0, ddof=0)\n",
    "    X_scaled = (X - means) / stds\n",
    "    return X_scaled, means, stds\n",
    "\n",
    "X_scaled, X_means, X_stds = feature_scale(X)\n",
    "print(\"Feature means:\", X_means)\n",
    "print(\"Feature stds:\", X_stds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d004f",
   "metadata": {},
   "source": [
    "### 6.1 Gradient Descent with and without Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f577e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Without scaling\n",
    "alpha_no_scale = 1e-4\n",
    "w0 = np.zeros(n)\n",
    "b0 = 0.0\n",
    "w_ns, b_ns, it_ns, cost_ns = gradient_descent(X, y, w0, b0, alpha_no_scale, 300)\n",
    "\n",
    "# With scaling\n",
    "alpha_scale = 1e-1  # we can use a larger learning rate after scaling\n",
    "w0_s = np.zeros(n)\n",
    "b0_s = 0.0\n",
    "w_s, b_s, it_s, cost_s = gradient_descent(X_scaled, y, w0_s, b0_s, alpha_scale, 300)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(it_ns[2:], cost_ns[2:], label=\"No scaling\")\n",
    "plt.plot(it_s[2:], cost_s[2:], label=\"With scaling\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost J(w,b)\")\n",
    "plt.title(\"Effect of Feature Scaling on Convergence\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2b9ee",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Feature Engineering\n",
    "\n",
    "Sometimes we can create **new features** that better capture the structure of the problem.\n",
    "\n",
    "Example for houses:\n",
    "- We know that the *area* of the lot is roughly `front_m × depth_m`.\n",
    "- We can create a new feature:\n",
    "  $$\n",
    "  \\text{area} = \\text{front\\_m} \\times \\text{depth\\_m}.\n",
    "  $$\n",
    "\n",
    "We will add this `area` feature to our design matrix and see its effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a6aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "area = data[\"front_m\"].to_numpy() * data[\"depth_m\"].to_numpy()\n",
    "data[\"area_m2\"] = area\n",
    "\n",
    "# New design matrix with the added engineered feature\n",
    "X_fe = np.column_stack([\n",
    "    data[\"size_m2\"].to_numpy(),\n",
    "    data[\"bedrooms\"].to_numpy(),\n",
    "    data[\"front_m\"].to_numpy(),\n",
    "    data[\"depth_m\"].to_numpy(),\n",
    "    data[\"area_m2\"].to_numpy(),\n",
    "])\n",
    "\n",
    "n_fe = X_fe.shape[1]\n",
    "X_fe_scaled, means_fe, stds_fe = feature_scale(X_fe)\n",
    "\n",
    "w0_fe = np.zeros(n_fe)\n",
    "b0_fe = 0.0\n",
    "alpha_fe = 1e-1\n",
    "\n",
    "w_fe, b_fe, it_fe, cost_fe = gradient_descent(X_fe_scaled, y, w0_fe, b0_fe, alpha_fe, 300)\n",
    "\n",
    "print(\"Learned parameters with engineered area feature:\")\n",
    "print(\"w_fe =\", w_fe)\n",
    "print(\"b_fe =\", b_fe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f874e020",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Polynomial Regression\n",
    "\n",
    "Linear regression can also model **non-linear relationships** by adding **polynomial features**.\n",
    "\n",
    "Example: if price depends non-linearly on size, we can add $x_{\\text{size}}^2$ as a new feature:\n",
    "$$\n",
    "f_{w,b}(x) = w_1 \\,\\text{size} + w_2 \\,\\text{size}^2 + b.\n",
    "$$\n",
    "\n",
    "This is still *linear in the parameters* $w_1, w_2, b$, so we can use the same algorithms.\n",
    "\n",
    "We will:\n",
    "1. Take just the `size_m2` feature.\n",
    "2. Create a second feature `size_m2^2`.\n",
    "3. Fit a polynomial regression model and visualize the curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "size_only = data[\"size_m2\"].to_numpy()\n",
    "y_price = data[\"price_k\"].to_numpy()\n",
    "\n",
    "X_poly = np.column_stack([size_only, size_only ** 2])\n",
    "X_poly_scaled, means_poly, stds_poly = feature_scale(X_poly)\n",
    "\n",
    "n_poly = X_poly_scaled.shape[1]\n",
    "w0_poly = np.zeros(n_poly)\n",
    "b0_poly = 0.0\n",
    "alpha_poly = 1e-1\n",
    "\n",
    "w_poly, b_poly, it_poly, cost_poly = gradient_descent(X_poly_scaled, y_price, w0_poly, b0_poly, alpha_poly, 500)\n",
    "\n",
    "print(\"Learned parameters for polynomial regression (size and size^2):\")\n",
    "print(\"w_poly =\", w_poly)\n",
    "print(\"b_poly =\", b_poly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f44d9c",
   "metadata": {},
   "source": [
    "### 8.1 Visualize Polynomial Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7898a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a smooth grid of sizes for plotting the curve\n",
    "size_grid = np.linspace(size_only.min(), size_only.max(), 200)\n",
    "X_grid = np.column_stack([size_grid, size_grid ** 2])\n",
    "\n",
    "# Apply same scaling as training\n",
    "X_grid_scaled = (X_grid - means_poly) / stds_poly\n",
    "y_grid_pred = predict(X_grid_scaled, w_poly, b_poly)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(size_only, y_price, label=\"Data\")\n",
    "plt.plot(size_grid, y_grid_pred, label=\"Polynomial fit\")\n",
    "plt.xlabel(\"Size (m^2)\")\n",
    "plt.ylabel(\"Price (thousands)\")\n",
    "plt.title(\"Polynomial Regression: Price vs Size\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5000a4",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Exercises (for You to Try)\n",
    "\n",
    "1. **Change the learning rate $\\alpha$** for the multifeature model:\n",
    "   - Try values like `1e-3`, `5e-4`, `5e-2` (with scaling).\n",
    "   - How does the convergence speed change? Does it diverge for some values?\n",
    "\n",
    "2. **Try different engineered features**:\n",
    "   - For example, try `size_per_bedroom = size_m2 / bedrooms` (be careful with division by zero).\n",
    "   - Add it as an extra column and see if the cost decreases faster or to a smaller value.\n",
    "\n",
    "3. **Compare models with and without the `area` feature**:\n",
    "   - Train two models: one with features `[size, bedrooms, front, depth]` and one with `[size, bedrooms, front, depth, area]`.\n",
    "   - Compare training cost and the scatter plot of true vs predicted prices.\n",
    "\n",
    "4. **Increase the polynomial degree**:\n",
    "   - Add a `size_m2^3` column and repeat the polynomial regression part.\n",
    "   - Does the fit improve? Does it start to look like it is overfitting the noise?\n",
    "\n",
    "5. **(Optional) Use a real dataset**:\n",
    "   - If you have a CSV file with real house data, load it with `pandas.read_csv`,\n",
    "   - Build the design matrix $X$ and target $y$,\n",
    "   - Apply the same steps: scaling, gradient descent, feature engineering, and polynomial regression.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
